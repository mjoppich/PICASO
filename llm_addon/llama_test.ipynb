{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba1f6e9-e517-46a6-9195-d7f63c47bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4649142-d99a-49da-be88-a1751a9df50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d73742b4a44602ae394fa5608e9605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BioMistral-7B-DARE-Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Download the GGUF model\n",
    "model_name = \"LoneStriker/BioMistral-7B-DARE-GGUF\"\n",
    "model_file = \"BioMistral-7B-DARE-Q4_K_M.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
    "model_path = hf_hub_download(model_name, filename=model_file, local_dir=\"/mnt/extproj/projekte/regulatory_networks/llm_addon/model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45fe8a1a-175a-408c-86dd-ca652a218008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from /mnt/extproj/projekte/regulatory_networks/llm_addon/model/BioMistral-7B-DARE-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 259\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = models\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      "...............................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'models', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL VARIABLES\n",
    "my_model_path = \"./model/BioMistral-7B-DARE-Q4_K_M.gguf\"\n",
    "CONTEXT_SIZE = 512\n",
    "\n",
    "\n",
    "# LOAD THE MODEL\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=2048,  # Context length to use\n",
    "    n_threads=16,            # Number of CPU threads to use\n",
    "    n_gpu_layers=0,        # Number of model layers to offload to GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fe74fb-2874-4414-90b3-6d25c09c0a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10bd448a-f2dc-46cc-853c-de4f5f5c24c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of information to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: The following genes are dysregulated in blood: IFI27, IFITM1, IFITM3, IFIT2, IFIT1, MT2A, IFI6, SIGLEC1, IFIT3, RSAD2, ISG15, LY6E, IFI44L, MX1. How are these genes connected and which molecular functions are altered?\n",
      "\n",
      "Do not repeat functions of single genes. Highlight genes for which a clinical trial is running or has been completed.\n",
      "\n",
      "Only return the helpful answer. Answer must be concise, detailed and well explained.\n",
      "Helpful answer:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     548.95 ms\n",
      "llama_print_timings:      sample time =      49.59 ms /   200 runs   (    0.25 ms per token,  4033.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7196.89 ms /    42 tokens (  171.35 ms per token,     5.84 tokens per second)\n",
      "llama_print_timings:        eval time =   14513.26 ms /   199 runs   (   72.93 ms per token,    13.71 tokens per second)\n",
      "llama_print_timings:       total time =   21882.43 ms /   241 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The genes you mentioned are all involved in the innate immune response to viral infection. They encode proteins that are part of the interferon (IFN) signaling pathway, which is a key component of the innate immune system's defense against viruses. The IFN signaling pathway involves the production and secretion of type I and III IFNs by infected cells, which then bind to receptors on neighboring cells and activate downstream signaling cascades that lead to the expression of genes involved in antiviral immunity. The genes you listed encode proteins that are either directly involved in this pathway (e.g., IFITM1, IFITM3, IFIT2, IFIT1, IFI6, ISG15, LY6E, IFI44L, MX1) or indirectly related to it (e.g., MT2A, RSAD2).\n"
     ]
    }
   ],
   "source": [
    "## Generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":-1,\n",
    "    \"stop\":[\"</s>\"],\n",
    "    \"echo\":False, # Echo the prompt in the output\n",
    "    \"top_k\":1, # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "    \"temperature\": 0.9\n",
    "}\n",
    "\n",
    "## Run inference\n",
    "geneList = (\"blood\", [\"IFI27\", \"IFITM1\", \"IFITM3\", \"IFIT2\", \"IFIT1\", \"MT2A\", \"IFI6\", \"SIGLEC1\", \"IFIT3\", \"RSAD2\", \"ISG15\", \"LY6E\", \"IFI44L\", \"MX1\"])\n",
    "#geneList = (\"kidney\", ['NOG', 'LHCGR', 'DIO2', 'NPR1', 'SLC8A1', 'NR2F1', 'EGFL7', 'JARID2', 'HJV', 'PLOD1', 'SHOX2', 'ADRB3', 'TBX3', 'ATP11B', 'COX6C', 'BMPR1A', 'APOC3', 'CALR', 'ATF2', 'TPO', 'FGF10', 'NCOR2', 'RBM19', 'LMBR1L', 'TRMT1', 'FOXH1', 'ELSPBP1', 'TBX18', 'ME3', 'RGMB', 'LEMD3', 'ACOT11', 'PCP2', 'TBX6', 'TBX20', 'RGMA', 'ANK2', 'BMP5', 'GATA5', 'ZFYVE16', 'SCGB1A1', 'SMAD9', 'SLC47A1', 'NOS3', 'HIPK1', 'SUMO1', 'DRAP1', 'BMP2', 'BMPR2', 'CHRDL1', 'TBX1', 'LYL1', 'NKX2-5', 'APOA2', 'GJA5', 'APOA4', 'SLC5A5', 'HMGA2', 'ACTA2', 'HIPK2', 'DHX9', 'CHRD', 'MOV10L1', 'ACVRL1', 'PKD1', 'HFE', 'TBX5', 'CCDC89', 'SERPINB3', 'PRKD3', 'SIRT1', 'PLA2G1B', 'ETV2', 'mir-513a', 'SRA1', 'mir-320a', 'mir-1', 'mir-214', 'mir-223', 'mir-185', 'mir-125b', 'mir-155', 'mir-137', 'mir-206', 'mir-4271', 'mir-122', 'mir-647', 'mir-335', 'mir-543', 'mir-19b', 'mir-92a', 'mir-106a', 'mir-96', 'mir-143', 'mir-145', 'mir-17', 'mir-18a', 'mir-19a', 'mir-20a', 'mir-21', 'mir-141', 'mir-181a', 'mir-27a', 'mir-200a', 'mir-200b'])\n",
    "\n",
    "genePrompt = \"The following genes are dysregulated in {}: {}. How are these genes connected and which molecular functions are altered?\".format(geneList[0],\", \".join(geneList[1]))\n",
    "\n",
    "prompt = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {}\n",
    "\n",
    "Do not repeat functions of single genes. Highlight genes for which a clinical trial is running or has been completed.\n",
    "\n",
    "Only return the helpful answer. Answer must be concise, detailed and well explained.\n",
    "Helpful answer:\n",
    "\"\"\".format(genePrompt)\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "\n",
    "## Unpack and the generated text from the LLM response dictionary and print it\n",
    "print(res[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "43c06fa9-774e-4d29-b738-b394c76a50b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use the following pieces of information to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "Question: In which diseases does Slfnlnc play a role? What is its mechanism?\n",
      "\n",
      "Do not repeat functions of single genes. Highlight genes for which a clinical trial is running or has been completed.\n",
      "\n",
      "Only return the helpful answer. Answer must be detailed and well explained.\n",
      "Helpful answer:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     548.95 ms\n",
      "llama_print_timings:      sample time =      70.31 ms /   278 runs   (    0.25 ms per token,  3954.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7571.03 ms /    60 tokens (  126.18 ms per token,     7.92 tokens per second)\n",
      "llama_print_timings:        eval time =   20410.28 ms /   277 runs   (   73.68 ms per token,    13.57 tokens per second)\n",
      "llama_print_timings:       total time =   28236.75 ms /   337 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slfnlnc is a long non-coding RNA (lncRNA) that plays a role in several diseases, including cancer, cardiovascular disease, and neurological disorders. In cancer, Slfnlnc has been shown to promote cell proliferation, migration, and invasion in various types of cancers, such as breast cancer, lung cancer, and colorectal cancer. Its mechanism involves the regulation of several genes involved in these processes, including cyclin D1, matrix metalloproteinase 2 (MMP-2), and vascular endothelial growth factor A (VEGF-A). In cardiovascular disease, Slfnlnc has been shown to regulate the expression of genes involved in atherosclerosis, such as interleukin-6 (IL-6) and tumor necrosis factor alpha (TNF-α), leading to the development of plaques. In neurological disorders, Slfnlnc has been shown to regulate the expression of genes involved in Alzheimer's disease, such as amyloid precursor protein (APP) and presenilin 1 (PSEN1). Its mechanism involves the regulation of several signaling pathways, including the Wnt/β-catenin pathway.\n"
     ]
    }
   ],
   "source": [
    "## Generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":-1,\n",
    "    \"stop\":[\"</s>\"],\n",
    "    \"echo\":False, # Echo the prompt in the output\n",
    "    \"top_k\":1, # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "    \"temperature\": 0.9\n",
    "}\n",
    "\n",
    "## Run inference\n",
    "geneList = (\"blood\", [\"IFI27\", \"IFITM1\", \"IFITM3\", \"IFIT2\", \"IFIT1\", \"MT2A\", \"IFI6\", \"SIGLEC1\", \"IFIT3\", \"RSAD2\", \"ISG15\", \"LY6E\", \"IFI44L\", \"MX1\"])\n",
    "#geneList = (\"kidney\", ['NOG', 'LHCGR', 'DIO2', 'NPR1', 'SLC8A1', 'NR2F1', 'EGFL7', 'JARID2', 'HJV', 'PLOD1', 'SHOX2', 'ADRB3', 'TBX3', 'ATP11B', 'COX6C', 'BMPR1A', 'APOC3', 'CALR', 'ATF2', 'TPO', 'FGF10', 'NCOR2', 'RBM19', 'LMBR1L', 'TRMT1', 'FOXH1', 'ELSPBP1', 'TBX18', 'ME3', 'RGMB', 'LEMD3', 'ACOT11', 'PCP2', 'TBX6', 'TBX20', 'RGMA', 'ANK2', 'BMP5', 'GATA5', 'ZFYVE16', 'SCGB1A1', 'SMAD9', 'SLC47A1', 'NOS3', 'HIPK1', 'SUMO1', 'DRAP1', 'BMP2', 'BMPR2', 'CHRDL1', 'TBX1', 'LYL1', 'NKX2-5', 'APOA2', 'GJA5', 'APOA4', 'SLC5A5', 'HMGA2', 'ACTA2', 'HIPK2', 'DHX9', 'CHRD', 'MOV10L1', 'ACVRL1', 'PKD1', 'HFE', 'TBX5', 'CCDC89', 'SERPINB3', 'PRKD3', 'SIRT1', 'PLA2G1B', 'ETV2', 'mir-513a', 'SRA1', 'mir-320a', 'mir-1', 'mir-214', 'mir-223', 'mir-185', 'mir-125b', 'mir-155', 'mir-137', 'mir-206', 'mir-4271', 'mir-122', 'mir-647', 'mir-335', 'mir-543', 'mir-19b', 'mir-92a', 'mir-106a', 'mir-96', 'mir-143', 'mir-145', 'mir-17', 'mir-18a', 'mir-19a', 'mir-20a', 'mir-21', 'mir-141', 'mir-181a', 'mir-27a', 'mir-200a', 'mir-200b'])\n",
    "\n",
    "genePrompt = \"In which diseases does Slfnlnc play a role? What is its mechanism?\"\n",
    "\n",
    "prompt = \"\"\"Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Question: {}\n",
    "\n",
    "Do not repeat functions of single genes. Highlight genes for which a clinical trial is running or has been completed.\n",
    "\n",
    "Only return the helpful answer. Answer must be detailed and well explained.\n",
    "Helpful answer:\n",
    "\"\"\".format(genePrompt)\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "\n",
    "## Unpack and the generated text from the LLM response dictionary and print it\n",
    "print(res[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bc71fa-ee83-4cac-8cd9-86604494ad8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regnetworks",
   "language": "python",
   "name": "regnetworks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
